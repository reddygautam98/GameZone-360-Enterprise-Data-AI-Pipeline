{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33b144a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df= pd.read_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20b4ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Step 1 Success: File loaded.\n",
      "Dimensions: 21864 rows, 14 columns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the specific path you provided\n",
    "file_path = r'C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx'\n",
    "\n",
    "# Check if file exists before trying to load\n",
    "if os.path.exists(file_path):\n",
    "    # Load the 'orders' sheet\n",
    "    try:\n",
    "        orders = pd.read_excel(file_path, sheet_name='orders')\n",
    "        print(\"‚úÖ Step 1 Success: File loaded.\")\n",
    "        print(f\"Dimensions: {len(orders)} rows, {len(orders.columns)} columns\")\n",
    "    except ValueError:\n",
    "        # Fallback if 'orders' sheet doesn't exist\n",
    "        print(\"‚ö†Ô∏è 'orders' sheet not found. Loading first sheet...\")\n",
    "        orders = pd.read_excel(file_path)\n",
    "        print(\"‚úÖ Step 1 Success: First sheet loaded.\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: File not found at {file_path}\")\n",
    "    orders = pd.DataFrame() # Create empty dataframe to prevent crashes in next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14c0eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Checking for Extra Columns ---\n",
      "‚ö†Ô∏è Issue Found: 2 extra columns detected.\n",
      "   Columns: ['Unnamed: 12', 'Unnamed: 13']\n"
     ]
    }
   ],
   "source": [
    "if not orders.empty:\n",
    "    print(\"--- Step 2: Checking for Extra Columns ---\")\n",
    "    extra_cols = [col for col in orders.columns if 'Unnamed' in col]\n",
    "\n",
    "    if extra_cols:\n",
    "        print(f\"‚ö†Ô∏è Issue Found: {len(extra_cols)} extra columns detected.\")\n",
    "        print(f\"   Columns: {extra_cols}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Status: Column structure is clean.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skip: Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6e84fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 3: Current Data Types ---\n",
      "USER_ID                            object\n",
      "ORDER_ID                           object\n",
      "PURCHASE_TS                        object\n",
      "SHIP_TS                    datetime64[ns]\n",
      "REFUND_TS                  datetime64[ns]\n",
      "PRODUCT_NAME                       object\n",
      "PRODUCT_ID                         object\n",
      "USD_PRICE                         float64\n",
      "PURCHASE_PLATFORM                  object\n",
      "MARKETING_CHANNEL                  object\n",
      "ACCOUNT_CREATION_METHOD            object\n",
      "COUNTRY_CODE                       object\n",
      "Unnamed: 12                       float64\n",
      "Unnamed: 13                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "if not orders.empty:\n",
    "    print(\"--- Step 3: Current Data Types ---\")\n",
    "    print(orders.dtypes)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skip: Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f2e6c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 4: Checking Logic Errors ---\n",
      "‚ö†Ô∏è Logic Error: 1997 orders have Ship Date < Purchase Date.\n",
      "‚ö†Ô∏è Logic Error: 29 orders have invalid prices ($0 or less).\n"
     ]
    }
   ],
   "source": [
    "if not orders.empty:\n",
    "    print(\"--- Step 4: Checking Logic Errors ---\")\n",
    "    \n",
    "    # Create a temporary copy to test logic safely\n",
    "    check_df = orders.copy()\n",
    "\n",
    "    # Temporarily convert columns to datetime/numeric for testing\n",
    "    date_cols = ['PURCHASE_TS', 'SHIP_TS', 'REFUND_TS']\n",
    "    for col in date_cols:\n",
    "        if col in check_df.columns:\n",
    "            check_df[col] = pd.to_datetime(check_df[col], errors='coerce')\n",
    "    \n",
    "    if 'USD_PRICE' in check_df.columns:\n",
    "        check_df['USD_PRICE'] = pd.to_numeric(check_df['USD_PRICE'], errors='coerce')\n",
    "\n",
    "        # Check A: Ship Date < Purchase Date\n",
    "        if 'SHIP_TS' in check_df.columns and 'PURCHASE_TS' in check_df.columns:\n",
    "            impossible = check_df[check_df['SHIP_TS'] < check_df['PURCHASE_TS']]\n",
    "            if not impossible.empty:\n",
    "                print(f\"‚ö†Ô∏è Logic Error: {len(impossible)} orders have Ship Date < Purchase Date.\")\n",
    "            else:\n",
    "                print(\"‚úÖ Date Logic: Valid.\")\n",
    "\n",
    "        # Check B: Prices <= 0\n",
    "        invalid_prices = check_df[check_df['USD_PRICE'] <= 0]\n",
    "        if not invalid_prices.empty:\n",
    "            print(f\"‚ö†Ô∏è Logic Error: {len(invalid_prices)} orders have invalid prices ($0 or less).\")\n",
    "        else:\n",
    "            print(\"‚úÖ Price Logic: Valid.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skip: Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f600b8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 5: Quality Check ---\n",
      "‚ö†Ô∏è Issue: Found 145 duplicate Order IDs.\n",
      "\n",
      "‚ö†Ô∏è Missing Data Detected:\n",
      "REFUND_TS                  18377\n",
      "USD_PRICE                      5\n",
      "MARKETING_CHANNEL             83\n",
      "ACCOUNT_CREATION_METHOD       83\n",
      "COUNTRY_CODE                  38\n",
      "Unnamed: 12                21864\n",
      "Unnamed: 13                21864\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if not orders.empty:\n",
    "    print(\"--- Step 5: Quality Check ---\")\n",
    "    \n",
    "    # Check Duplicates\n",
    "    if 'ORDER_ID' in orders.columns:\n",
    "        dup_ids = orders['ORDER_ID'].duplicated().sum()\n",
    "        if dup_ids > 0:\n",
    "            print(f\"‚ö†Ô∏è Issue: Found {dup_ids} duplicate Order IDs.\")\n",
    "        else:\n",
    "            print(\"‚úÖ Duplicates: None found.\")\n",
    "            \n",
    "    # Check Missing Values\n",
    "    missing = orders.isnull().sum()\n",
    "    missing_only = missing[missing > 0]\n",
    "    \n",
    "    if not missing_only.empty:\n",
    "        print(\"\\n‚ö†Ô∏è Missing Data Detected:\")\n",
    "        print(missing_only)\n",
    "    else:\n",
    "        print(\"‚úÖ Missing Data: None.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skip: Data not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30c28968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Columns:\n",
      "['user_id', 'order_id', 'purchase_ts', 'ship_ts', 'refund_ts', 'product_name', 'product_id', 'usd_price', 'purchase_platform', 'marketing_channel', 'account_creation_method', 'country_code', 'unnamed:_12', 'unnamed:_13']\n"
     ]
    }
   ],
   "source": [
    "# Standardize column names\n",
    "orders.columns = orders.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Verify the changes\n",
    "print(\"\\nUpdated Columns:\")\n",
    "print(orders.columns.tolist())\n",
    "\n",
    "# Save back to the same Excel file (overwrite)\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d0806de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Data Types:\n",
      "user_id                            object\n",
      "order_id                           object\n",
      "purchase_ts                datetime64[ns]\n",
      "ship_ts                    datetime64[ns]\n",
      "refund_ts                  datetime64[ns]\n",
      "product_name                       object\n",
      "product_id                         object\n",
      "usd_price                         float64\n",
      "purchase_platform                category\n",
      "marketing_channel                category\n",
      "account_creation_method          category\n",
      "country_code                       object\n",
      "unnamed:_12                       float64\n",
      "unnamed:_13                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 1. Fix Dates (The \"Wrong\" columns)\n",
    "date_cols = ['purchase_ts', 'ship_ts', 'refund_ts']\n",
    "for col in date_cols:\n",
    "    orders[col] = pd.to_datetime(orders[col], errors='coerce')\n",
    "\n",
    "# 2. Optimize Categories (Optional but Recommended)\n",
    "cat_cols = ['purchase_platform', 'marketing_channel', 'account_creation_method']\n",
    "for col in cat_cols:\n",
    "    orders[col] = orders[col].astype('category')\n",
    "\n",
    "# 3. Verify the Fix\n",
    "print(\"\\nUpdated Data Types:\")\n",
    "print(orders.dtypes)\n",
    "\n",
    "# 4. Save back to the same Excel file (overwrite)\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f4fbdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unnamed columns only if they exist\n",
    "unnamed_cols = ['Unnamed: 12', 'Unnamed: 13']\n",
    "existing_cols = [col for col in unnamed_cols if col in orders.columns]\n",
    "\n",
    "if existing_cols:\n",
    "    orders.drop(columns=existing_cols, inplace=True)\n",
    "\n",
    "# Save back to the same Excel file (overwrite)\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0abe6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Row Count: 21864\n",
      "‚úÖ SAVED 34 invalid/missing rows to: C:\\Users\\reddy\\Downloads\\stockholder_review_data.xlsx\n",
      "‚úÖ UPDATED Main File with 21830 clean rows at: C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# SPLIT & SAVE: REVIEW DATA + CLEAN MAIN DATA\n",
    "# ==========================================\n",
    "\n",
    "# 1. Define File Paths\n",
    "main_file_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "review_file_path = r\"C:\\Users\\reddy\\Downloads\\stockholder_review_data.xlsx\"\n",
    "\n",
    "# 2. Load the Data\n",
    "try:\n",
    "    df = pd.read_excel(main_file_path)\n",
    "    print(f\"Original Row Count: {len(df)}\")\n",
    "\n",
    "    # 3. Identify Price Column\n",
    "    if 'USD_PRICE' in df.columns:\n",
    "        price_col = 'USD_PRICE'\n",
    "    elif 'usd_price' in df.columns:\n",
    "        price_col = 'usd_price'\n",
    "    else:\n",
    "        raise ValueError(\"Could not find Price column.\")\n",
    "\n",
    "    # 4. Create filter for invalid or missing prices\n",
    "    invalid_mask = (df[price_col].isna()) | (df[price_col] <= 0)\n",
    "\n",
    "    # 5. Split the Data\n",
    "    error_data = df[invalid_mask].copy()   # Data for Stakeholders (invalid + missing)\n",
    "    clean_data = df[~invalid_mask].copy()  # Data for Analysis (valid only)\n",
    "\n",
    "    # 6. Save the Files\n",
    "    # Requirement A: Save error data for review\n",
    "    error_data.to_excel(review_file_path, index=False)\n",
    "    print(f\"‚úÖ SAVED {len(error_data)} invalid/missing rows to: {review_file_path}\")\n",
    "\n",
    "    # Requirement B: Overwrite main file with clean data\n",
    "    clean_data.to_excel(main_file_path, index=False)\n",
    "    print(f\"‚úÖ UPDATED Main File with {len(clean_data)} clean rows at: {main_file_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "720a4477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after fix: 0\n",
      "marketing_channel\n",
      "direct          17412\n",
      "email            3251\n",
      "affiliate         715\n",
      "social media      322\n",
      "Unknown            83\n",
      "unknown            47\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Updated file saved at: C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\n"
     ]
    }
   ],
   "source": [
    "# Replace missing values in marketing_channel with \"Unknown\"\n",
    "if 'marketing_channel' in orders.columns:\n",
    "    orders['marketing_channel'] = orders['marketing_channel'].fillna('Unknown')\n",
    "\n",
    "# Verify the fix\n",
    "print(\"Missing values after fix:\", orders['marketing_channel'].isna().sum())\n",
    "print(orders['marketing_channel'].value_counts())\n",
    "\n",
    "# Save back to the same Excel file (overwrite)\n",
    "orders.to_excel(file_path, index=False)\n",
    "print(f\"‚úÖ Updated file saved at: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29f0ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize 'Unknown' entries in marketing_channel column\n",
    "if 'marketing_channel' in orders.columns:\n",
    "    orders['marketing_channel'] = orders['marketing_channel'].replace({'unknown': 'Unknown'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2af5d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marketing_channel\n",
      "direct          17412\n",
      "email            3251\n",
      "affiliate         715\n",
      "social media      322\n",
      "Unknown           130\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(orders['marketing_channel'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57cc18e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaned row count: 21685\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Drop duplicate Order IDs, keeping the first occurrence\n",
    "if 'order_id' in orders.columns:\n",
    "    orders = orders.drop_duplicates(subset='order_id', keep='first')\n",
    "\n",
    "# Optional: Verify how many rows remain\n",
    "print(f\"‚úÖ Cleaned row count: {len(orders)}\")\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7443df9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reddy\\AppData\\Local\\Temp\\ipykernel_5768\\3525237821.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  orders['purchase_ts'] = pd.to_datetime(orders['purchase_ts'], errors='coerce')\n",
      "C:\\Users\\reddy\\AppData\\Local\\Temp\\ipykernel_5768\\3525237821.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  orders['ship_ts'] = pd.to_datetime(orders['ship_ts'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median shipping delay: 2.0 days\n",
      "Remaining logic errors: 0\n",
      "‚úÖ Updated file saved with corrected ship dates.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assume orders DataFrame is already loaded\n",
    "# Ensure datetime conversion\n",
    "orders['purchase_ts'] = pd.to_datetime(orders['purchase_ts'], errors='coerce')\n",
    "orders['ship_ts'] = pd.to_datetime(orders['ship_ts'], errors='coerce')\n",
    "\n",
    "# ‚úÖ Step 1: Calculate shipping delay (in days) for valid rows\n",
    "valid_mask = orders['ship_ts'] >= orders['purchase_ts']\n",
    "valid_delays = (orders.loc[valid_mask, 'ship_ts'] - orders.loc[valid_mask, 'purchase_ts']).dt.days\n",
    "\n",
    "# ‚úÖ Step 2: Compute median delay\n",
    "median_delay = valid_delays.median()\n",
    "print(f\"Median shipping delay: {median_delay} days\")\n",
    "\n",
    "# ‚úÖ Step 3: Fix invalid rows (ship_ts < purchase_ts)\n",
    "invalid_mask = orders['ship_ts'] < orders['purchase_ts']\n",
    "orders.loc[invalid_mask, 'ship_ts'] = orders.loc[invalid_mask, 'purchase_ts'] + pd.to_timedelta(median_delay, unit='D')\n",
    "\n",
    "# ‚úÖ Step 4: Verify fix\n",
    "print(\"Remaining logic errors:\", (orders['ship_ts'] < orders['purchase_ts']).sum())\n",
    "\n",
    "# ‚úÖ Step 5: Save back to Excel\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n",
    "print(\"‚úÖ Updated file saved with corrected ship dates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f9f3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Verification Passed: No orders have Ship Date earlier than Purchase Date.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Verify that no orders have Ship Date earlier than Purchase Date\n",
    "invalid_count = (orders['ship_ts'] < orders['purchase_ts']).sum()\n",
    "\n",
    "if invalid_count == 0:\n",
    "    print(\"‚úÖ Verification Passed: No orders have Ship Date earlier than Purchase Date.\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Verification Failed: {invalid_count} orders still have Ship Date < Purchase Date.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c85fda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reddy\\AppData\\Local\\Temp\\ipykernel_5768\\2617751766.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  orders['account_creation_method'] = orders['account_creation_method'].fillna('Unknown')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated file saved with corrected ship dates.\n"
     ]
    }
   ],
   "source": [
    "orders['account_creation_method'] = orders['account_creation_method'].fillna('Unknown')\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n",
    "print(\"‚úÖ Updated file saved with corrected ship dates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "271b9477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after fix: 0\n",
      "Rows labeled as 'Unknown': 83\n",
      "account_creation_method\n",
      "desktop    16309\n",
      "mobile      4220\n",
      "unknown      729\n",
      "tablet       319\n",
      "Unknown       83\n",
      "tv            25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Verify replacement of missing values in account_creation_method\n",
    "missing_count = orders['account_creation_method'].isna().sum()\n",
    "unknown_count = (orders['account_creation_method'] == 'Unknown').sum()\n",
    "\n",
    "print(f\"Missing values after fix: {missing_count}\")\n",
    "print(f\"Rows labeled as 'Unknown': {unknown_count}\")\n",
    "print(orders['account_creation_method'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bc0494e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reddy\\AppData\\Local\\Temp\\ipykernel_5768\\964557365.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  orders['country_code'] = orders['country_code'].fillna('Unknown')\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Replace missing values in country_code with \"Unknown\"\n",
    "orders['country_code'] = orders['country_code'].fillna('Unknown')\n",
    "\n",
    "# ‚úÖ Save the changes back to the main file\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9add7304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after fix: 0\n",
      "Rows labeled as 'Unknown': 38\n",
      "country_code\n",
      "US    10218\n",
      "GB     1790\n",
      "CA      942\n",
      "AU      887\n",
      "DE      845\n",
      "      ...  \n",
      "RE        1\n",
      "MZ        1\n",
      "MH        1\n",
      "MD        1\n",
      "LC        1\n",
      "Name: count, Length: 151, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Verify replacement of missing values in country_code\n",
    "missing_count = orders['country_code'].isna().sum()\n",
    "unknown_count = (orders['country_code'] == 'Unknown').sum()\n",
    "\n",
    "print(f\"Missing values after fix: {missing_count}\")\n",
    "print(f\"Rows labeled as 'Unknown': {unknown_count}\")\n",
    "print(orders['country_code'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcf22603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\reddy\\AppData\\Local\\Temp\\ipykernel_5768\\2281601090.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  orders.drop(columns=unnamed_cols, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Drop all columns that start with \"Unnamed\"\n",
    "unnamed_cols = [col for col in orders.columns if col.strip().startswith(\"Unnamed\")]\n",
    "orders.drop(columns=unnamed_cols, inplace=True)\n",
    "\n",
    "# ‚úÖ Save the updated DataFrame to the main file\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "563a17d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nintendo Switch' 'Sony PlayStation 5 Bundle' '27in 4K gaming monitor'\n",
      " 'JBL Quantum 100 Gaming Headset' 'Dell Gaming Mouse'\n",
      " 'Acer Nitro V Gaming Laptop' 'Lenovo IdeaPad Gaming 3'\n",
      " 'Razer Pro Gaming Headset' '27inches 4k gaming monitor']\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Display all unique product names \n",
    "print(orders['product_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae55f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS: Merged 61 rows of '27inches 4k gaming monitor' into '27in 4K gaming monitor'.\n",
      "   New total count for '27in 4K gaming monitor': 4678\n",
      "\n",
      "Current Unique Products:\n",
      "['Nintendo Switch' 'Sony PlayStation 5 Bundle' '27in 4K gaming monitor'\n",
      " 'JBL Quantum 100 Gaming Headset' 'Dell Gaming Mouse'\n",
      " 'Acer Nitro V Gaming Laptop' 'Lenovo IdeaPad Gaming 3'\n",
      " 'Razer Pro Gaming Headset']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the Data\n",
    "# Note: Using your local path. If reading the CSV directly, change read_excel to read_csv\n",
    "file_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "\n",
    "try:\n",
    "    # Attempt to read as Excel first\n",
    "    df = pd.read_excel(file_path)\n",
    "except:\n",
    "    # Fallback to read as CSV if it's actually a CSV file disguised as xlsx\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except:\n",
    "        # Fallback for sheet name issues\n",
    "        df = pd.read_excel(file_path, sheet_name=0)\n",
    "\n",
    "# 2. Standardize Column Names (Best Practice)\n",
    "# This ensures code works whether columns are 'PRODUCT_NAME' or 'product_name'\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "\n",
    "# 3. Standardize Product Name\n",
    "bad_name = \"27inches 4k gaming monitor\"\n",
    "good_name = \"27in 4K gaming monitor\"\n",
    "\n",
    "if 'product_name' in df.columns:\n",
    "    # Check how many exist before change\n",
    "    count_before = len(df[df['product_name'] == bad_name])\n",
    "    \n",
    "    if count_before > 0:\n",
    "        # Apply the fix\n",
    "        df.loc[df['product_name'] == bad_name, 'product_name'] = good_name\n",
    "        \n",
    "        # 4. Save Changes\n",
    "        # If it was a CSV, save as CSV. If Excel, save as Excel.\n",
    "        if file_path.endswith('.csv'):\n",
    "            df.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            df.to_excel(file_path, index=False)\n",
    "        \n",
    "        # 5. Verification Report\n",
    "        print(f\"‚úÖ SUCCESS: Merged {count_before} rows of '{bad_name}' into '{good_name}'.\")\n",
    "        print(f\"   New total count for '{good_name}': {len(df[df['product_name'] == good_name])}\")\n",
    "    else:\n",
    "        print(f\"‚ÑπÔ∏è No rows found with name: '{bad_name}'. Data might already be clean.\")\n",
    "\n",
    "    # Show current unique products\n",
    "    print(\"\\nCurrent Unique Products:\")\n",
    "    print(df['product_name'].unique())\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Error: Column 'product_name' not found. Check your file headers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbda3fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Data Quality Audit ---\n",
      "\n",
      "--- DATA QUALITY SUMMARY ---\n",
      "dq_tier\n",
      "Perfect    21642\n",
      "Good          43\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- SAMPLE ISSUES FOUND ---\n",
      "              order_id  dq_score                       dq_issues\n",
      "526   06ee8b82fbc46119        90  Invalid format in country_code\n",
      "671   08feac8f0a020345        90  Invalid format in country_code\n",
      "1043  0dc92d0562552247        90  Invalid format in country_code\n",
      "1047  0dda212aaea69940        80             Missing purchase_ts\n",
      "3581  2fa5682923166358        90  Invalid format in country_code\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataQualityAuditor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        # Standardize column names for consistent checking\n",
    "        self.df.columns = self.df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "        \n",
    "        # Initialize 'issues' column to store error messages\n",
    "        self.issues = pd.Series([\"\"] * len(self.df), index=self.df.index, dtype='object')\n",
    "        \n",
    "        # Initialize 'score' column at 100 (Perfect)\n",
    "        self.score = pd.Series([100] * len(self.df), index=self.df.index)\n",
    "\n",
    "    def _log_issue(self, mask, message, penalty=10):\n",
    "        \"\"\"\n",
    "        Core Logic: If a row fails a check (mask=True), \n",
    "        append the error message and deduct points.\n",
    "        \"\"\"\n",
    "        if mask.any():\n",
    "            idx = self.df.index[mask]\n",
    "            \n",
    "            # 1. Deduct Score\n",
    "            self.score.loc[idx] -= penalty\n",
    "            \n",
    "            # 2. Append Error Message (Robust string concatenation)\n",
    "            new_vals = self.issues.loc[idx].apply(lambda x: f\"{x}, {message}\" if x else message)\n",
    "            self.issues.loc[idx] = new_vals\n",
    "\n",
    "    def check_completeness(self, critical_columns):\n",
    "        \"\"\"Flag rows with missing values in key columns.\"\"\"\n",
    "        for col in critical_columns:\n",
    "            if col in self.df.columns:\n",
    "                mask = self.df[col].isnull()\n",
    "                self._log_issue(mask, f\"Missing {col}\", penalty=20)\n",
    "\n",
    "    def check_validity_numeric(self, col, min_val=None, max_val=None):\n",
    "        \"\"\"Flag rows with numeric values out of bounds (e.g., Price <= 0).\"\"\"\n",
    "        if col in self.df.columns:\n",
    "            # Ensure numeric type\n",
    "            self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "            \n",
    "            if min_val is not None:\n",
    "                mask = self.df[col] < min_val\n",
    "                self._log_issue(mask, f\"{col} too low (<{min_val})\", penalty=100) # Critical Error\n",
    "            if max_val is not None:\n",
    "                mask = self.df[col] > max_val\n",
    "                self._log_issue(mask, f\"{col} too high (>{max_val})\", penalty=10)\n",
    "\n",
    "    def check_logic_dates(self):\n",
    "        \"\"\"Check logical order of dates (e.g., Ship Date cannot be before Purchase Date).\"\"\"\n",
    "        date_cols = ['purchase_ts', 'ship_ts', 'refund_ts']\n",
    "        for col in date_cols:\n",
    "             if col in self.df.columns:\n",
    "                self.df[col] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "\n",
    "        # Rule 1: Ship Date < Purchase Date\n",
    "        if 'purchase_ts' in self.df.columns and 'ship_ts' in self.df.columns:\n",
    "            mask = (self.df['ship_ts'].notnull()) & (self.df['purchase_ts'].notnull()) & \\\n",
    "                   (self.df['ship_ts'] < self.df['purchase_ts'])\n",
    "            self._log_issue(mask, \"Ship Date before Purchase\", penalty=50)\n",
    "\n",
    "        # Rule 2: Refund Date < Purchase Date\n",
    "        if 'purchase_ts' in self.df.columns and 'refund_ts' in self.df.columns:\n",
    "            mask = (self.df['refund_ts'].notnull()) & (self.df['purchase_ts'].notnull()) & \\\n",
    "                   (self.df['refund_ts'] < self.df['purchase_ts'])\n",
    "            self._log_issue(mask, \"Refund Date before Purchase\", penalty=50)\n",
    "            \n",
    "    def check_format_regex(self, col, regex):\n",
    "        \"\"\"Check text formats using Regex (e.g., Country Code must be 2 uppercase letters).\"\"\"\n",
    "        if col in self.df.columns:\n",
    "            # Check only non-null values against regex\n",
    "            mask = self.df[col].notnull() & ~self.df[col].astype(str).str.match(regex)\n",
    "            self._log_issue(mask, f\"Invalid format in {col}\", penalty=10)\n",
    "\n",
    "    def run_audit(self):\n",
    "        print(\"--- Starting Data Quality Audit ---\")\n",
    "        \n",
    "        # 1. Define Critical Checks\n",
    "        self.check_completeness(['order_id', 'user_id', 'product_name', 'usd_price', 'purchase_ts'])\n",
    "        self.check_validity_numeric('usd_price', min_val=0.01) # Price must be > 0\n",
    "        self.check_logic_dates()\n",
    "        self.check_format_regex('country_code', regex=r'^[A-Z]{2}$') # e.g., 'US', 'IN'\n",
    "        \n",
    "        # 2. Compile Results\n",
    "        self.df['dq_issues'] = self.issues\n",
    "        self.df['dq_score'] = self.score.clip(lower=0) # Minimum score is 0\n",
    "        \n",
    "        # 3. Assign Tier\n",
    "        conditions = [\n",
    "            (self.df['dq_score'] == 100),\n",
    "            (self.df['dq_score'] >= 80),\n",
    "            (self.df['dq_score'] < 80)\n",
    "        ]\n",
    "        choices = ['Perfect', 'Good', 'Critical']\n",
    "        self.df['dq_tier'] = np.select(conditions, choices, default='Critical')\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# 1. Load Data\n",
    "file_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "except:\n",
    "    df = pd.read_excel(file_path, sheet_name=0)\n",
    "\n",
    "# 2. Run Auditor\n",
    "auditor = DataQualityAuditor(df)\n",
    "report_df = auditor.run_audit()\n",
    "\n",
    "# 3. Save Report\n",
    "report_df.to_csv(r\"C:\\Users\\reddy\\Downloads\\gamezone_data_quality_report.csv\", index=False)\n",
    "\n",
    "# 4. Display Summary\n",
    "print(\"\\n--- DATA QUALITY SUMMARY ---\")\n",
    "print(report_df['dq_tier'].value_counts())\n",
    "print(\"\\n--- SAMPLE ISSUES FOUND ---\")\n",
    "print(report_df[report_df['dq_score'] < 100][['order_id', 'dq_score', 'dq_issues']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ab51567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Added sheet 'Invalid_Country_Code' with 38 rows.\n",
      "üìÑ Added sheet 'Missing_Purchase_Date' with 5 rows.\n",
      "‚úÖ Review File Saved: C:\\Users\\reddy\\Downloads\\stackholder_review_data.xlsx\n",
      "‚úÖ Main File Updated (Clean Data Only): 21642 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# MOVE ERRORS TO SEPARATE SHEETS (REVIEW FILE)\n",
    "# ==========================================\n",
    "\n",
    "# 1. Define Paths\n",
    "main_file_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "# Note: Using the path you provided (check for typo 'stackholder' vs 'stockholder')\n",
    "review_file_path = r\"C:\\Users\\reddy\\Downloads\\stackholder_review_data.xlsx\"\n",
    "\n",
    "# 2. Load Data\n",
    "try:\n",
    "    df = pd.read_excel(main_file_path)\n",
    "except:\n",
    "    df = pd.read_excel(main_file_path, sheet_name=0)\n",
    "\n",
    "# Standardize columns\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "\n",
    "# 3. Identify Errors by Category\n",
    "# A. Invalid Country Codes (Not 2 chars)\n",
    "country_mask = ~df['country_code'].astype(str).str.match(r'^[A-Z]{2}$')\n",
    "country_errors = df[country_mask].copy()\n",
    "\n",
    "# B. Missing Purchase Dates\n",
    "date_mask = df['purchase_ts'].isnull()\n",
    "date_errors = df[date_mask].copy()\n",
    "\n",
    "# C. Invalid Prices (<= 0) - Just in case any remain\n",
    "price_mask = df['usd_price'] <= 0\n",
    "price_errors = df[price_mask].copy()\n",
    "\n",
    "# 4. Identify Clean Data (Remove rows with ANY error)\n",
    "all_errors_mask = country_mask | date_mask | price_mask\n",
    "clean_df = df[~all_errors_mask].copy()\n",
    "\n",
    "# 5. Save to Excel with Multiple Sheets\n",
    "# We use pd.ExcelWriter to write to multiple sheets in one file\n",
    "try:\n",
    "    with pd.ExcelWriter(review_file_path, engine='openpyxl') as writer:\n",
    "        \n",
    "        # Write Country Errors\n",
    "        if not country_errors.empty:\n",
    "            country_errors.to_excel(writer, sheet_name='Invalid_Country_Code', index=False)\n",
    "            print(f\"üìÑ Added sheet 'Invalid_Country_Code' with {len(country_errors)} rows.\")\n",
    "        \n",
    "        # Write Date Errors\n",
    "        if not date_errors.empty:\n",
    "            date_errors.to_excel(writer, sheet_name='Missing_Purchase_Date', index=False)\n",
    "            print(f\"üìÑ Added sheet 'Missing_Purchase_Date' with {len(date_errors)} rows.\")\n",
    "            \n",
    "        # Write Price Errors\n",
    "        if not price_errors.empty:\n",
    "            price_errors.to_excel(writer, sheet_name='Invalid_Prices', index=False)\n",
    "            print(f\"üìÑ Added sheet 'Invalid_Prices' with {len(price_errors)} rows.\")\n",
    "            \n",
    "        if country_errors.empty and date_errors.empty and price_errors.empty:\n",
    "            print(\"‚úÖ No errors found to move!\")\n",
    "            \n",
    "    print(f\"‚úÖ Review File Saved: {review_file_path}\")\n",
    "\n",
    "    # 6. Update Main File (Keep only Clean Data)\n",
    "    clean_df.to_excel(main_file_path, index=False)\n",
    "    print(f\"‚úÖ Main File Updated (Clean Data Only): {len(clean_df)} rows.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving files: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80a6e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING VERIFICATION ---\n",
      "\n",
      "‚úÖ Review File Found: C:\\Users\\reddy\\Downloads\\stackholder_review_data.xlsx\n",
      "   Sheets found: ['Invalid_Country_Code', 'Missing_Purchase_Date']\n",
      "   - Sheet 'Invalid_Country_Code': Contains 38 rows.\n",
      "   - Sheet 'Missing_Purchase_Date': Contains 5 rows.\n",
      "\n",
      "‚úÖ Main File Found: C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\n",
      "   ‚úÖ SUCCESS: Main file is 100% CLEAN.\n",
      "      - Invalid Country Codes: 0\n",
      "      - Missing Dates: 0\n",
      "      - Invalid Prices: 0\n",
      "   Total Valid Rows: 21642\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Define Paths (Using the same filenames as before)\n",
    "main_file_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "review_file_path = r\"C:\\Users\\reddy\\Downloads\\stackholder_review_data.xlsx\"\n",
    "\n",
    "print(\"--- STARTING VERIFICATION ---\")\n",
    "\n",
    "# 2. Verify Review File (The \"Bad\" Data)\n",
    "if os.path.exists(review_file_path):\n",
    "    print(f\"\\n‚úÖ Review File Found: {review_file_path}\")\n",
    "    \n",
    "    # Load all sheets to check them\n",
    "    xls = pd.ExcelFile(review_file_path)\n",
    "    sheet_names = xls.sheet_names\n",
    "    print(f\"   Sheets found: {sheet_names}\")\n",
    "    \n",
    "    # Check specific sheets\n",
    "    expected_sheets = ['Invalid_Country_Code', 'Missing_Purchase_Date']\n",
    "    for sheet in expected_sheets:\n",
    "        if sheet in sheet_names:\n",
    "            df_sheet = pd.read_excel(xls, sheet_name=sheet)\n",
    "            print(f\"   - Sheet '{sheet}': Contains {len(df_sheet)} rows.\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Warning: Sheet '{sheet}' is missing (maybe no errors of this type existed?)\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: Review file not found at {review_file_path}\")\n",
    "\n",
    "# 3. Verify Main File (The \"Clean\" Data)\n",
    "if os.path.exists(main_file_path):\n",
    "    print(f\"\\n‚úÖ Main File Found: {main_file_path}\")\n",
    "    try:\n",
    "        df_main = pd.read_excel(main_file_path)\n",
    "    except:\n",
    "        df_main = pd.read_excel(main_file_path, sheet_name=0)\n",
    "        \n",
    "    # Standardize columns for checking\n",
    "    df_main.columns = df_main.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "    \n",
    "    # RE-RUN THE AUDIT LOGIC TO PROVE IT IS CLEAN\n",
    "    \n",
    "    # Check 1: Invalid Country Codes\n",
    "    bad_country = df_main[~df_main['country_code'].astype(str).str.match(r'^[A-Z]{2}$')]\n",
    "    \n",
    "    # Check 2: Missing Dates\n",
    "    bad_dates = df_main[df_main['purchase_ts'].isnull()]\n",
    "    \n",
    "    # Check 3: Invalid Prices\n",
    "    bad_prices = df_main[df_main['usd_price'] <= 0] if 'usd_price' in df_main.columns else []\n",
    "    \n",
    "    if len(bad_country) == 0 and len(bad_dates) == 0 and len(bad_prices) == 0:\n",
    "        print(f\"   ‚úÖ SUCCESS: Main file is 100% CLEAN.\")\n",
    "        print(f\"      - Invalid Country Codes: 0\")\n",
    "        print(f\"      - Missing Dates: 0\")\n",
    "        print(f\"      - Invalid Prices: 0\")\n",
    "        print(f\"   Total Valid Rows: {len(df_main)}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå FAILURE: Main file still has errors!\")\n",
    "        print(f\"      - Invalid Country Codes: {len(bad_country)}\")\n",
    "        print(f\"      - Missing Dates: {len(bad_dates)}\")\n",
    "        print(f\"      - Invalid Prices: {len(bad_prices)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Error: Main file not found at {main_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58e0a0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ STARTING PIPELINE QUALITY GATE...\n",
      "Target File: C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\n",
      "\n",
      "\n",
      "--- GATE RESULTS ---\n",
      "[PASS] Schema Check Passed: All 6 columns present.\n",
      "[PASS] Completeness Verified: order_id is 100% populated.\n",
      "[PASS] Completeness Verified: usd_price is 100% populated.\n",
      "[PASS] Completeness Verified: purchase_ts is 100% populated.\n",
      "[PASS] Completeness Verified: country_code is 100% populated.\n",
      "[PASS] Logic Verified: All prices are positive.\n",
      "[PASS] Logic Verified: Temporal ordering (Purchase -> Ship) is correct.\n",
      "[PASS] Format Verified: All country codes follow ISO 2-char standard.\n",
      "\n",
      "==============================\n",
      "‚úÖ PIPELINE STATUS: GREEN LIGHT (GO)\n",
      "   Data is clean, consistent, and ready for ingestion.\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class PipelineQualityGate:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.df = None\n",
    "        self.status = True\n",
    "        self.report = []\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads data and standardizes columns for validation.\"\"\"\n",
    "        if not os.path.exists(self.file_path):\n",
    "            self._log(f\"CRITICAL: File not found at {self.file_path}\", level=\"FAIL\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            try:\n",
    "                self.df = pd.read_excel(self.file_path)\n",
    "            except:\n",
    "                self.df = pd.read_excel(self.file_path, sheet_name=0)\n",
    "            \n",
    "            # Standardize columns (Best Practice for Pipelines)\n",
    "            self.df.columns = self.df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self._log(f\"CRITICAL: Failed to read file. Error: {e}\", level=\"FAIL\")\n",
    "            return False\n",
    "\n",
    "    def _log(self, message, level=\"INFO\"):\n",
    "        \"\"\"Internal logger.\"\"\"\n",
    "        self.report.append(f\"[{level}] {message}\")\n",
    "        if level == \"FAIL\":\n",
    "            self.status = False\n",
    "\n",
    "    def validate_schema(self, required_columns):\n",
    "        \"\"\"Ensures all necessary columns exist.\"\"\"\n",
    "        missing = [col for col in required_columns if col not in self.df.columns]\n",
    "        if missing:\n",
    "            self._log(f\"Schema Mismatch! Missing columns: {missing}\", level=\"FAIL\")\n",
    "        else:\n",
    "            self._log(f\"Schema Check Passed: All {len(required_columns)} columns present.\", level=\"PASS\")\n",
    "\n",
    "    def validate_completeness(self, critical_columns):\n",
    "        \"\"\"Ensures zero nulls in critical fields.\"\"\"\n",
    "        for col in critical_columns:\n",
    "            if col in self.df.columns:\n",
    "                null_count = self.df[col].isnull().sum()\n",
    "                if null_count > 0:\n",
    "                    self._log(f\"Completeness Failure: {col} has {null_count} missing values.\", level=\"FAIL\")\n",
    "                else:\n",
    "                    self._log(f\"Completeness Verified: {col} is 100% populated.\", level=\"PASS\")\n",
    "\n",
    "    def validate_business_logic(self):\n",
    "        \"\"\"Checks advanced logic constraints.\"\"\"\n",
    "        # 1. Price Integrity\n",
    "        if 'usd_price' in self.df.columns:\n",
    "            neg_prices = self.df[self.df['usd_price'] <= 0]\n",
    "            if not neg_prices.empty:\n",
    "                self._log(f\"Logic Failure: Found {len(neg_prices)} rows with Price <= 0\", level=\"FAIL\")\n",
    "            else:\n",
    "                self._log(\"Logic Verified: All prices are positive.\", level=\"PASS\")\n",
    "\n",
    "        # 2. Date Integrity (Ship vs Purchase)\n",
    "        if 'ship_ts' in self.df.columns and 'purchase_ts' in self.df.columns:\n",
    "            # Ensure datetime type\n",
    "            self.df['ship_ts'] = pd.to_datetime(self.df['ship_ts'], errors='coerce')\n",
    "            self.df['purchase_ts'] = pd.to_datetime(self.df['purchase_ts'], errors='coerce')\n",
    "            \n",
    "            bad_dates = self.df[(self.df['ship_ts'] < self.df['purchase_ts'])]\n",
    "            if not bad_dates.empty:\n",
    "                self._log(f\"Logic Failure: {len(bad_dates)} orders shipped before purchase.\", level=\"FAIL\")\n",
    "            else:\n",
    "                self._log(\"Logic Verified: Temporal ordering (Purchase -> Ship) is correct.\", level=\"PASS\")\n",
    "\n",
    "        # 3. Format Integrity (Country Code)\n",
    "        if 'country_code' in self.df.columns:\n",
    "            bad_codes = self.df[~self.df['country_code'].astype(str).str.match(r'^[A-Z]{2}$')]\n",
    "            if not bad_codes.empty:\n",
    "                self._log(f\"Format Failure: {len(bad_codes)} invalid country codes found.\", level=\"FAIL\")\n",
    "            else:\n",
    "                self._log(\"Format Verified: All country codes follow ISO 2-char standard.\", level=\"PASS\")\n",
    "\n",
    "    def run_gate(self):\n",
    "        print(\"üöÄ STARTING PIPELINE QUALITY GATE...\")\n",
    "        print(f\"Target File: {self.file_path}\\n\")\n",
    "        \n",
    "        if self.load_data():\n",
    "            # Define your Pipeline Contract (What MUST be true)\n",
    "            self.validate_schema(['order_id', 'user_id', 'product_name', 'usd_price', 'purchase_ts', 'country_code'])\n",
    "            self.validate_completeness(['order_id', 'usd_price', 'purchase_ts', 'country_code'])\n",
    "            self.validate_business_logic()\n",
    "        \n",
    "        print(\"\\n--- GATE RESULTS ---\")\n",
    "        for line in self.report:\n",
    "            print(line)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*30)\n",
    "        if self.status:\n",
    "            print(\"‚úÖ PIPELINE STATUS: GREEN LIGHT (GO)\")\n",
    "            print(\"   Data is clean, consistent, and ready for ingestion.\")\n",
    "        else:\n",
    "            print(\"üõë PIPELINE STATUS: RED LIGHT (NO GO)\")\n",
    "            print(\"   Critical errors detected. Do not proceed to ingestion.\")\n",
    "        print(\"=\"*30)\n",
    "\n",
    "# ==========================================\n",
    "# EXECUTION\n",
    "# ==========================================\n",
    "file_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "gate = PipelineQualityGate(file_path)\n",
    "gate.run_gate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc08620e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Hashes ---\n",
      "‚úÖ Created column: user_id_sha256\n",
      "‚úÖ Created column: product_id_sha256\n",
      "‚úÖ File saved successfully: C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\n",
      "\n",
      "--- Sample Output ---\n",
      "    user_id                                     user_id_sha256 product_id  \\\n",
      "0  2c06175e  683bccf9ecda62f60ff3db0d82c80d1f263ca6cc4a4391...       e682   \n",
      "1  ee8e5bc2  80fef5010f0a72ae4faf76fd450f129c53c5e0c5bc0352...       e682   \n",
      "2  9eb4efe0  ff196d75d2d5288c9bb0832015f4de8b2a32377925467e...       8d0d   \n",
      "3  cac7cbaf  d0b3daf7daa3df1d414f9757f83700cdd0eb872f620d4c...       54ed   \n",
      "4  6b0230bc  f7fc635c932b51cd9cca47eb9e98eae9ad8b0fa66f89e2...       8d0d   \n",
      "\n",
      "                                   product_id_sha256  \n",
      "0  3855c12e3c82068e038acb3c8fafbbbb5f9c5d88613517...  \n",
      "1  3855c12e3c82068e038acb3c8fafbbbb5f9c5d88613517...  \n",
      "2  0813cab06a74897cfaa92e87bd20a3771e88ff14239074...  \n",
      "3  ab46419b315378675d2530b35d6d346d8edeaa30ff9b19...  \n",
      "4  0813cab06a74897cfaa92e87bd20a3771e88ff14239074...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "# 1. Load Data\n",
    "file_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "except:\n",
    "    df = pd.read_excel(file_path, sheet_name=0)\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "\n",
    "# 2. Define the Hashing Function\n",
    "def generate_sha256(value):\n",
    "    \"\"\"\n",
    "    Converts any value to a string and returns its SHA-256 hash.\n",
    "    Standardizes data by ignoring types (int vs string).\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    # Encode string to bytes, then hash\n",
    "    return hashlib.sha256(str(value).encode('utf-8')).hexdigest()\n",
    "\n",
    "print(\"--- Processing Hashes ---\")\n",
    "\n",
    "# 3. Apply to User ID\n",
    "if 'user_id' in df.columns:\n",
    "    df['user_id_sha256'] = df['user_id'].apply(generate_sha256)\n",
    "    print(\"‚úÖ Created column: user_id_sha256\")\n",
    "\n",
    "# 4. Apply to Product ID\n",
    "if 'product_id' in df.columns:\n",
    "    df['product_id_sha256'] = df['product_id'].apply(generate_sha256)\n",
    "    print(\"‚úÖ Created column: product_id_sha256\")\n",
    "\n",
    "# 5. Save the Updated File\n",
    "df.to_excel(file_path, index=False)\n",
    "print(f\"‚úÖ File saved successfully: {file_path}\")\n",
    "\n",
    "# 6. Preview the Transformation\n",
    "print(\"\\n--- Sample Output ---\")\n",
    "print(df[['user_id', 'user_id_sha256', 'product_id', 'product_id_sha256']].head())\n",
    "\n",
    "# ‚úÖ Save the updated DataFrame to the main file\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ff8974a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ‚úÖ Save the updated DataFrame to the main file\n",
    "orders.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39983c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Processed.\n",
      "   - Total Orders: 21685\n",
      "   - Refunded Orders: 3444\n",
      "   - Refund Rate: 15.9%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\")\n",
    "\n",
    "# Standardize columns\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SOLUTION 1: Create the 'is_refunded' Flag (Best for Analysis)\n",
    "# ---------------------------------------------------------\n",
    "# Logic: If refund_ts has a value -> 1 (True). If it is null -> 0 (False).\n",
    "df['is_refunded'] = df['refund_ts'].notnull().astype(int)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SOLUTION 2: Handling the Date Column Itself (Best for Display)\n",
    "# ---------------------------------------------------------\n",
    "# Option A: Keep as NaT (Pandas default) - Best for time-series math\n",
    "df['refund_ts'] = pd.to_datetime(df['refund_ts'], errors='coerce')\n",
    "\n",
    "# Option B: Fill with placeholder (Only for exporting to BI tools like Tableau/PowerBI)\n",
    "# Some tools hate Null dates, so we give them a dummy date far in the future or past.\n",
    "# df['refund_ts_clean'] = df['refund_ts'].fillna(pd.Timestamp(\"1900-01-01\"))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHECK YOUR METRICS\n",
    "# ---------------------------------------------------------\n",
    "refund_rate = df['is_refunded'].mean() * 100\n",
    "print(f\"‚úÖ Data Processed.\")\n",
    "print(f\"   - Total Orders: {len(df)}\")\n",
    "print(f\"   - Refunded Orders: {df['is_refunded'].sum()}\")\n",
    "print(f\"   - Refund Rate: {refund_rate:.1f}%\")\n",
    "\n",
    "# Save the enriched file\n",
    "df.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c0ffebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Hashes ---\n",
      "‚úÖ Created column: user_id_sha256\n",
      "‚úÖ Created column: product_id_sha256\n",
      "‚úÖ File saved successfully: C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\n",
      "\n",
      "--- Sample Output ---\n",
      "    user_id                                     user_id_sha256 product_id  \\\n",
      "0  2c06175e  683bccf9ecda62f60ff3db0d82c80d1f263ca6cc4a4391...       e682   \n",
      "1  ee8e5bc2  80fef5010f0a72ae4faf76fd450f129c53c5e0c5bc0352...       e682   \n",
      "2  9eb4efe0  ff196d75d2d5288c9bb0832015f4de8b2a32377925467e...       8d0d   \n",
      "3  cac7cbaf  d0b3daf7daa3df1d414f9757f83700cdd0eb872f620d4c...       54ed   \n",
      "4  6b0230bc  f7fc635c932b51cd9cca47eb9e98eae9ad8b0fa66f89e2...       8d0d   \n",
      "\n",
      "                                   product_id_sha256  \n",
      "0  3855c12e3c82068e038acb3c8fafbbbb5f9c5d88613517...  \n",
      "1  3855c12e3c82068e038acb3c8fafbbbb5f9c5d88613517...  \n",
      "2  0813cab06a74897cfaa92e87bd20a3771e88ff14239074...  \n",
      "3  ab46419b315378675d2530b35d6d346d8edeaa30ff9b19...  \n",
      "4  0813cab06a74897cfaa92e87bd20a3771e88ff14239074...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "# 1. Load Data\n",
    "file_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "except:\n",
    "    df = pd.read_excel(file_path, sheet_name=0)\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "\n",
    "# 2. Define the Hashing Function\n",
    "def generate_sha256(value):\n",
    "    \"\"\"\n",
    "    Converts any value to a string and returns its SHA-256 hash.\n",
    "    Standardizes data by ignoring types (int vs string).\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    return hashlib.sha256(str(value).encode('utf-8')).hexdigest()\n",
    "\n",
    "print(\"--- Processing Hashes ---\")\n",
    "\n",
    "# 3. Apply to User ID\n",
    "if 'user_id' in df.columns:\n",
    "    df['user_id_sha256'] = df['user_id'].apply(generate_sha256)\n",
    "    print(\"‚úÖ Created column: user_id_sha256\")\n",
    "\n",
    "# 4. Apply to Product ID\n",
    "if 'product_id' in df.columns:\n",
    "    df['product_id_sha256'] = df['product_id'].apply(generate_sha256)\n",
    "    print(\"‚úÖ Created column: product_id_sha256\")\n",
    "\n",
    "# 5. Save the Updated File\n",
    "df.to_excel(file_path, index=False)\n",
    "print(f\"‚úÖ File saved successfully: {file_path}\")\n",
    "\n",
    "# 6. Preview the Transformation (safe check)\n",
    "preview_cols = [col for col in ['user_id', 'user_id_sha256', 'product_id', 'product_id_sha256'] if col in df.columns]\n",
    "print(\"\\n--- Sample Output ---\")\n",
    "print(df[preview_cols].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46a84352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Processed.\n",
      "   - Total Orders: 21685\n",
      "   - Refunded Orders: 3444\n",
      "   - Refund Rate: 15.9%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\")\n",
    "\n",
    "# Standardize columns\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SOLUTION 1: Create the 'is_refunded' Flag (Best for Analysis)\n",
    "# ---------------------------------------------------------\n",
    "df['is_refunded'] = df['refund_ts'].notnull().astype(int)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SOLUTION 2: Handling the Date Column Itself (Best for Display)\n",
    "# ---------------------------------------------------------\n",
    "df['refund_ts'] = pd.to_datetime(df['refund_ts'], errors='coerce')\n",
    "# Optional: df['refund_ts_clean'] = df['refund_ts'].fillna(pd.Timestamp(\"1900-01-01\"))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHECK YOUR METRICS\n",
    "# ---------------------------------------------------------\n",
    "refund_rate = df['is_refunded'].mean() * 100\n",
    "print(f\"‚úÖ Data Processed.\")\n",
    "print(f\"   - Total Orders: {len(df)}\")\n",
    "print(f\"   - Refunded Orders: {df['is_refunded'].sum()}\")\n",
    "print(f\"   - Refund Rate: {refund_rate:.1f}%\")\n",
    "\n",
    "# Save the enriched file (fixed syntax)\n",
    "df.to_excel(r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a57cba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying Features to All Rows ---\n",
      "--- Splitting Data ---\n",
      "‚úÖ Main File Saved: 21642 rows\n",
      "‚úÖ Stakeholder Review File Saved with new columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# MASTER UPDATE SCRIPT\n",
    "# ==========================================\n",
    "# 1. Define Paths\n",
    "main_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "review_path = r\"C:\\Users\\reddy\\Downloads\\stockholder_review_data.xlsx\"\n",
    "\n",
    "# 2. Load Data\n",
    "try:\n",
    "    df = pd.read_excel(main_path)\n",
    "except:\n",
    "    df = pd.read_excel(main_path, sheet_name=0)\n",
    "\n",
    "# Standardize Columns\n",
    "df.columns = df.columns.str.lower().str.strip().str.replace(' ', '_')\n",
    "\n",
    "# 3. APPLY ALL FEATURES (To the ENTIRE dataset)\n",
    "print(\"--- Applying Features to All Rows ---\")\n",
    "\n",
    "# A. SHA-256 Hashing\n",
    "def get_sha256(val):\n",
    "    if pd.isna(val): return None\n",
    "    return hashlib.sha256(str(val).encode('utf-8')).hexdigest()\n",
    "\n",
    "if 'user_id' in df.columns: \n",
    "    df['user_id_sha256'] = df['user_id'].apply(get_sha256)\n",
    "if 'product_id' in df.columns: \n",
    "    df['product_id_sha256'] = df['product_id'].apply(get_sha256)\n",
    "\n",
    "# B. Operational & Flags\n",
    "if 'ship_ts' in df.columns and 'purchase_ts' in df.columns:\n",
    "    df['days_to_ship'] = (df['ship_ts'] - df['purchase_ts']).dt.days\n",
    "    df['is_late_shipment'] = (df['days_to_ship'] > 3).astype(int)\n",
    "\n",
    "# C. Refund Logic\n",
    "if 'refund_ts' in df.columns:\n",
    "    df['is_refunded'] = df['refund_ts'].notnull().astype(int)\n",
    "\n",
    "# D. Lifecycle (Sorting required)\n",
    "if 'user_id' in df.columns and 'purchase_ts' in df.columns:\n",
    "    df = df.sort_values(by=['user_id', 'purchase_ts'])\n",
    "    df['order_rank'] = df.groupby('user_id').cumcount() + 1\n",
    "    df['is_returning'] = (df['order_rank'] > 1).astype(int)\n",
    "\n",
    "# 4. SPLIT DATA (Clean vs Error)\n",
    "print(\"--- Splitting Data ---\")\n",
    "\n",
    "# Define Error Logic\n",
    "mask_country = ~df['country_code'].astype(str).str.match(r'^[A-Z]{2}$')\n",
    "mask_dates = df['purchase_ts'].isnull()\n",
    "mask_prices = (df['usd_price'].isnull()) | (df['usd_price'] <= 0)\n",
    "\n",
    "# Combine for \"Clean\" Data\n",
    "total_error_mask = mask_country | mask_dates | mask_prices\n",
    "clean_df = df[~total_error_mask].copy()\n",
    "\n",
    "# 5. SAVE UPDATES\n",
    "\n",
    "# A. Update MAIN FILE (Clean Only)\n",
    "clean_df.to_excel(main_path, index=False)\n",
    "print(f\"‚úÖ Main File Saved: {len(clean_df)} rows\")\n",
    "\n",
    "# B. Update REVIEW FILE (Errors + New Columns)\n",
    "# We write multiple sheets for easier review\n",
    "with pd.ExcelWriter(review_path, engine='openpyxl') as writer:\n",
    "    if mask_country.any():\n",
    "        df[mask_country].to_excel(writer, sheet_name='Invalid_Country', index=False)\n",
    "    if mask_dates.any():\n",
    "        df[mask_dates].to_excel(writer, sheet_name='Missing_Dates', index=False)\n",
    "    if mask_prices.any():\n",
    "        df[mask_prices].to_excel(writer, sheet_name='Invalid_Prices', index=False)\n",
    "\n",
    "print(f\"‚úÖ Stakeholder Review File Saved with new columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0007970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dropped columns ['unnamed:_12', 'unnamed:_13']\n",
      "‚úÖ File saved: C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the Main File\n",
    "file_path = r\"C:\\Users\\reddy\\Downloads\\gamezone-orders-data (1).xlsx\"\n",
    "try:\n",
    "    df = pd.read_excel(file_path)\n",
    "except:\n",
    "    df = pd.read_excel(file_path, sheet_name=0)\n",
    "\n",
    "# 2. Drop the Columns\n",
    "# We use errors='ignore' so the script doesn't crash if they are already gone\n",
    "cols_to_drop = ['unnamed:_12', 'unnamed:_13']\n",
    "df.drop(columns=cols_to_drop, errors='ignore', inplace=True)\n",
    "\n",
    "# 3. Save Changes\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Dropped columns {cols_to_drop}\")\n",
    "print(f\"‚úÖ File saved: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gamezone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
